{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCD9kXIjCiew"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L55zqzl6CmHe"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWPl0km9--vE"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"input_data\"\n",
    "today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "file_name = f\"/kaggle/input/my-input-dataset/{today_str}_input_data.csv\"\n",
    "Clean_data = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "E_PSw7gLBcLR",
    "outputId": "7855f94e-7355-41bb-b264-4f548ad35c0a"
   },
   "outputs": [],
   "source": [
    "Clean_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsb0P4qFMzMs",
    "outputId": "b9cc80dc-c072-4a7a-d61e-02d889f54bf8"
   },
   "outputs": [],
   "source": [
    "print(Clean_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqPq68w-CzZr"
   },
   "outputs": [],
   "source": [
    "Clean_data = Clean_data.drop('Time_x', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofTrVDyXB6u3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create 'group' column (every 4 rows = 1 hour)\n",
    "Clean_data['group'] = Clean_data.index // 4\n",
    "\n",
    "# Group by 'group' and take averages for all MW columns\n",
    "hourly_Data = Clean_data.groupby('group').agg({\n",
    "    'Date': 'first',\n",
    "    'Hour': 'first',\n",
    "    'Purchase Bid (MW)': 'mean',\n",
    "    'Total Sell Bid (MW)': 'mean',\n",
    "    'Sell bid Solar (MW)': 'mean',\n",
    "    'Sell bid Non-Solar (MW)': 'mean',\n",
    "    'Sell bid Hydro (MW)': 'mean',\n",
    "    'MCV Total (MW)': 'mean',\n",
    "    'MCV Solar (MW)': 'mean',\n",
    "    'MCV Non-Solar (MW)': 'mean',\n",
    "    'MCV Hydro (MW)': 'mean',\n",
    "    'MCP (Rs/MWh) ': 'mean',  # keep mean for MCP as well\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# --- Adjust Hour column (1â€“24, reset daily) ---\n",
    "hourly_Data['Hour'] = (hourly_Data.groupby('Date').cumcount() % 24) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMtqZKvzKMnl",
    "outputId": "7c830f13-a147-4f9f-d97a-7bb49407e9b8"
   },
   "outputs": [],
   "source": [
    "hourly_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lneGHRFfNekW",
    "outputId": "871ee339-a541-4c5d-b47a-f1587ec2672b"
   },
   "outputs": [],
   "source": [
    "print(hourly_Data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL_l7xtcO-Q6"
   },
   "outputs": [],
   "source": [
    "hourly_Data.to_csv('df_Hourly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wn9N9NMFQnuh",
    "outputId": "9ee4fb30-aaf2-4154-e273-eb3aa309ccf6"
   },
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime objects\n",
    "hourly_Data['Date'] = pd.to_datetime(hourly_Data['Date'])\n",
    "\n",
    "# Combine 'Date' and 'Hour' to create a new 'Datetime' column\n",
    "# We need to adjust the Hour since it's 1-24 and datetime expects 0-23 for hour\n",
    "hourly_Data['Datetime'] = hourly_Data['Date'] + pd.to_timedelta(hourly_Data['Hour'] - 1, unit='h')\n",
    "\n",
    "# Display the first few rows with the new 'Datetime' column\n",
    "print(hourly_Data[['Date', 'Hour', 'Datetime']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPy-YoI_RIqc"
   },
   "outputs": [],
   "source": [
    "# Drop the original 'Date' and 'Hour' columns\n",
    "hourly_Data = hourly_Data.drop(['Date'], axis=1)\n",
    "\n",
    "# Set the 'Datetime' column as the index\n",
    "hourly_Data = hourly_Data.set_index('Datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "FH8zgBPMSuyH",
    "outputId": "aa8173bd-1202-4f15-d5d5-a09437c55cc3"
   },
   "outputs": [],
   "source": [
    "hourly_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn40OFr8NW8X"
   },
   "source": [
    "##feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjhM643Yzhy8"
   },
   "outputs": [],
   "source": [
    "daily_avg = hourly_Data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEPuf4LGIG6a"
   },
   "outputs": [],
   "source": [
    "daily_avg['Day_of_Week'] = daily_avg.index.dayofweek\n",
    "daily_avg['Month'] = daily_avg.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcIQTX5OQGAB"
   },
   "outputs": [],
   "source": [
    "# Lag-1 (Yesterday's values)\n",
    "daily_avg['MCP_lag_1'] = daily_avg['MCP (Rs/MWh) '].shift(24)\n",
    "daily_avg['MCV_lag_1'] = daily_avg['MCV Total (MW)'].shift(24)\n",
    "\n",
    "daily_avg['MCP_avg'] = daily_avg['MCP (Rs/MWh) '].rolling(window=24).mean()  # Daily average for MCP\n",
    "daily_avg['MCV_avg'] = daily_avg['MCV Total (MW)'].rolling(window=24).mean()  # Daily average for MCV\n",
    "\n",
    "daily_avg['MCP_7d_avg'] = daily_avg['MCP (Rs/MWh) '].rolling(window=24 * 7).mean()  # Weekly average for MCP\n",
    "daily_avg['MCV_7d_avg'] = daily_avg['MCV Total (MW)'].rolling(window=24 * 7).mean()  # Weekly average for MCV\n",
    "\n",
    "# daily_avg['monthly_avg_MCP'] = daily_avg['MCP (Rs/MWh) '].rolling(window=24 * 30).mean()  # Monthly average for MCP\n",
    "# daily_avg['monthly_avg_MCV'] = daily_avg['MCV Total (MW)'].rolling(window=24 * 30).mean()  # Monthly average for MCV\n",
    "\n",
    "# Drop NaN values after adding new columns\n",
    "daily_avg.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "YzUyiAMLS4lB",
    "outputId": "bfc922bc-49bb-4b2c-d98a-494fb1e1ee86"
   },
   "outputs": [],
   "source": [
    "daily_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "rhS7ayvDS5tj",
    "outputId": "c8bad7bc-2109-4c07-842c-2100f99289c5"
   },
   "outputs": [],
   "source": [
    "cols = daily_avg.columns.tolist()\n",
    "# Remove 'Day_of_Week' and 'Month' from their current position and insert them at the beginning\n",
    "cols.remove('Day_of_Week')\n",
    "cols.remove('Month')\n",
    "cols = ['Day_of_Week', 'Month'] + cols\n",
    "daily_avg = daily_avg[cols]\n",
    "\n",
    "daily_avg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD4P9QGSXem9"
   },
   "source": [
    "##correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "id": "BEfhylEpTwk9",
    "outputId": "0407226d-6157-4d15-929f-8c7b32c93a0b"
   },
   "outputs": [],
   "source": [
    "correlation_matrix = daily_avg.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VJnULGomUKvO",
    "outputId": "52f47044-93ad-42b2-c3c4-27bbba09d514"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Daily Average Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThaW-iY5mqQv"
   },
   "source": [
    "#differant dataframes with strong correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx0rsyUarWV6"
   },
   "outputs": [],
   "source": [
    "daily_avg['Year'] = daily_avg.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFMOimypqo25"
   },
   "outputs": [],
   "source": [
    "main_df1 = daily_avg.drop(['Sell bid Solar (MW)', 'Sell bid Non-Solar (MW)', 'Sell bid Hydro (MW)', 'MCV Solar (MW)', 'MCV Non-Solar (MW)', 'MCV Hydro (MW)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "skpFv6yIQRf1",
    "outputId": "f5e60599-7938-4998-bb2f-210c5d537ab3"
   },
   "outputs": [],
   "source": [
    "main_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POhsHmxWrjUg"
   },
   "outputs": [],
   "source": [
    "MCP_df = daily_avg.drop(['Year','Purchase Bid (MW)','Total Sell Bid (MW)', 'Sell bid Solar (MW)', 'Sell bid Non-Solar (MW)', 'Sell bid Hydro (MW)', 'MCV Total (MW)', 'MCV Solar (MW)', 'MCV Non-Solar (MW)', 'MCV Hydro (MW)', 'MCV_lag_1', 'MCV_avg','MCV_7d_avg'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf5jYoGyJgYi"
   },
   "outputs": [],
   "source": [
    "MCP_df = MCP_df.drop(['Hour','Month','Day_of_Week'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "L2pbIFE3Qv_9",
    "outputId": "8d46797f-08ab-4e68-92fd-770dff3b56b6"
   },
   "outputs": [],
   "source": [
    "MCP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEQd-LRO7QAn"
   },
   "outputs": [],
   "source": [
    "MCV_df = daily_avg.drop(['MCP (Rs/MWh) ','MCV Solar (MW)','MCV Non-Solar (MW)','MCV Hydro (MW)','Hour','Sell bid Hydro (MW)','Sell bid Non-Solar (MW)','Sell bid Solar (MW)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edRwfoT-srwa"
   },
   "outputs": [],
   "source": [
    "MCV_df = daily_avg.drop(['MCP (Rs/MWh) ', 'MCP_lag_1', 'MCP_avg','MCP_7d_avg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f2Apqi1GSPz"
   },
   "outputs": [],
   "source": [
    "MCV_df=MCV_df.drop(['Hour','Month','Day_of_Week'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "Oxi5TXnxE0k3",
    "outputId": "d950eed2-d52b-46c1-a715-532a797c2215"
   },
   "outputs": [],
   "source": [
    "MCV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "g_DVbMS0tN2u",
    "outputId": "68780ab4-ef1f-40b0-baa4-9367631a9ba9"
   },
   "outputs": [],
   "source": [
    "# prompt: graph of correlation matrix of MCP_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming MCP_df is already defined as in your provided code\n",
    "correlation_matrix_mcp = MCP_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_mcp, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of MCP_df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhkJueV1tK4z",
    "outputId": "a83f850d-e0a7-48fd-bef9-4be4bb8e118f"
   },
   "outputs": [],
   "source": [
    "daily_avg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T3enWq8_2BE"
   },
   "source": [
    "##preparation for model MCP with MCP correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbfXTon7_2BF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Aa56GIgLPXt"
   },
   "outputs": [],
   "source": [
    "last_month = MCP_df.iloc[-24*31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fD63i-nZLT63"
   },
   "outputs": [],
   "source": [
    "MCP_df = MCP_df.iloc[:-24*31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "FKQxvuGeMJXx",
    "outputId": "22df2f01-8bb0-42db-f419-5f0fe3e0944e"
   },
   "outputs": [],
   "source": [
    "MCP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiZtJGU7MN42"
   },
   "outputs": [],
   "source": [
    "MCP_df= MCP_df.drop(['MCP_avg','MCP_7d_avg'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "2Omc7r8fMjSC",
    "outputId": "c6239c3f-fd9d-4679-9d4f-7708589d34c5"
   },
   "outputs": [],
   "source": [
    "MCP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE5IwnuL_2BF"
   },
   "outputs": [],
   "source": [
    "X_df = MCP_df\n",
    "y_df = MCP_df[['MCP (Rs/MWh) ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_sChgWX_2BG",
    "outputId": "b0b8d13a-f555-450b-f73d-32413858b87e"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_df)\n",
    "joblib.dump(scaler_X, f'{today_str}_month_mcp_scaler_X.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0C7lvqv_2BG",
    "outputId": "08792db8-7476-4a8c-e33a-6b2c3c13134c"
   },
   "outputs": [],
   "source": [
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y_df)\n",
    "joblib.dump(scaler_y, f'{today_str}_month_mcp_scaler_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJI_RoV7_2BH"
   },
   "outputs": [],
   "source": [
    "X_data = np.array(X_scaled)\n",
    "y_data = np.array(y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BnuCsNI_2BI",
    "outputId": "70e25334-e636-471e-f37a-f430820abdaa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(X_data, y_data, sequence_length, prediction_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X_data) - sequence_length - prediction_length + 1):\n",
    "        sequences.append(X_data[i:i + sequence_length])\n",
    "        labels.append(y_data[i + sequence_length:i + sequence_length + prediction_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 24*30\n",
    "prediction_length = 24*30\n",
    "\n",
    "# Assuming X_data and y_data are already defined and contain the hourly data\n",
    "X, y = create_sequences(X_data, y_data, sequence_length, prediction_length)\n",
    "\n",
    "# Check the shape of the output\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfGWBYxd_2BJ",
    "outputId": "fa5432d6-0d9b-4e19-a6df-1d5cb3a6caa8"
   },
   "outputs": [],
   "source": [
    "split_index = int(0.9 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "print(\"Training Data Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKtsiebT_nxS"
   },
   "source": [
    "## Model for MCP correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0054dhEW_nxT"
   },
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "output_steps = 24*30\n",
    "output_units = output_steps * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSJfG9bbSee-"
   },
   "source": [
    "##simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "id": "DF41nD9W_nxU",
    "outputId": "3e169f13-2193-4c30-ae3b-88f33f3d71a8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "model_lstm = Sequential()\n",
    "#model_lstm.add(LSTM(256, return_sequences=True, input_shape=(sequence_length, num_features)))\n",
    "#model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(LSTM(128, return_sequences=True, input_shape=(sequence_length, num_features)))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(LSTM(64, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(LSTM(32, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(Dense(output_units, activation='linear'))  # MCP and MCV combined output\n",
    "\n",
    "# Compile the Model\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Model Summary\n",
    "#model_lstm.summary()\n",
    "\n",
    "# Train the Model\n",
    "history_lstm = model_lstm.fit(X_train, y_train.reshape(-1, output_units), epochs=50, batch_size=16,\n",
    "                              validation_split=0.3, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the Model\n",
    "train_loss_lstm = model_lstm.evaluate(X_train, y_train.reshape(-1, output_units))\n",
    "test_loss_lstm = model_lstm.evaluate(X_test, y_test.reshape(-1, output_units))\n",
    "print(f'Train Loss (LSTM): {train_loss_lstm}')\n",
    "print(f'Test Loss (LSTM): {test_loss_lstm}')\n",
    "\n",
    "# Plot the Training and Validation Loss for LSTM\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss (LSTM)')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss (LSTM)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model_lstm.save('mcv_mcp_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aaZY6cI-dFZh",
    "outputId": "0441ced5-9bce-4ac9-9f40-6c50ad63a4d3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scale Input Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Define the Optimized LSTM Model Architecture\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Bidirectional(LSTM(128, return_sequences=True, input_shape=(sequence_length, num_features))))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(64, activation='relu'))  # Additional Dense Layer for Better Representation\n",
    "model_lstm.add(Dense(output_units, activation='linear'))  # MCP and MCV combined output\n",
    "\n",
    "# Compile the Model with Modified Learning Rate and Optimizer\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Add Callbacks: EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_scaled, y_train.reshape(-1, output_units),\n",
    "    epochs=100,\n",
    "    batch_size=32,  # Adjusted Batch Size\n",
    "    validation_split=0.3,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "train_loss_lstm = model_lstm.evaluate(X_train_scaled, y_train.reshape(-1, output_units))\n",
    "test_loss_lstm = model_lstm.evaluate(X_test_scaled, y_test.reshape(-1, output_units))\n",
    "\n",
    "print(f'Train Loss (LSTM): {train_loss_lstm}')\n",
    "print(f'Test Loss (LSTM): {test_loss_lstm}')\n",
    "\n",
    "# Plot the Training and Validation Loss for LSTM\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss (LSTM)')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss (LSTM)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model_lstm.save('optimized_mcv_mcp_lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUaL9L6pcHl4",
    "outputId": "fad000ea-42c7-41c8-ffc6-e4c33cb88c32"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values to get the original scale\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))  # Use scaler_y for y_test\n",
    "y_pred_lstm_original = scaler_y.inverse_transform(y_pred_lstm.reshape(-1, 1))  # Use scaler_y for predictions\n",
    "\n",
    "# Calculate the R-squared score\n",
    "r2_lstm = r2_score(y_test_original, y_pred_lstm_original)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_lstm = mean_squared_error(y_test_original, y_pred_lstm_original)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"LSTM - R-squared: {r2_lstm}, MSE: {mse_lstm}, RMSE: {rmse_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "B4wud77fobgU",
    "outputId": "d232d5a5-54f7-4d2c-801a-88f80913479a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test_original and y_pred_lstm_original are already defined from your previous code\n",
    "\n",
    "# Select the last 672 entries (representing the last week)\n",
    "last_week_actual = y_test_original[-24*30:]\n",
    "last_week_predicted = y_pred_lstm_original[-24*30:]\n",
    "\n",
    "# Plot the graph for the last week\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(last_week_actual, label='Actual Values')\n",
    "plt.plot(last_week_predicted, label='Predicted Values')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Scaled Values')\n",
    "plt.title('Actual vs. Predicted Values for the Last Week (X_test)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON8M--wJIjiw"
   },
   "source": [
    "#MCV Month Ahead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIKzPbwiLrrZ"
   },
   "source": [
    "##preparation for model MCV with MCV correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n10lhMkLrrZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66uWuiiMLrra"
   },
   "outputs": [],
   "source": [
    "X_df = MCV_df\n",
    "y_df = MCV_df[['MCV Total (MW)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-p1Pek_aiXK",
    "outputId": "862065d7-5b2f-4b86-9d52-f3c0672a9f9b"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_df)\n",
    "joblib.dump(scaler_X, f'{today_str}_mcv_month_scaler_X.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcEe6OFFan9b",
    "outputId": "76581957-49fb-4a4c-9ad6-592eb7e65991"
   },
   "outputs": [],
   "source": [
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y_df)\n",
    "joblib.dump(scaler_y, f'{today_str}_mcv_month_scaler_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Peat7K7RLrrb"
   },
   "outputs": [],
   "source": [
    "X_data = np.array(X_scaled)\n",
    "y_data = np.array(y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z24tUktsLrrc",
    "outputId": "01f63b86-4f57-42fb-d950-bd81fa2ecacb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(X_data, y_data, sequence_length, prediction_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X_data) - sequence_length - prediction_length + 1):\n",
    "        sequences.append(X_data[i:i + sequence_length])\n",
    "        labels.append(y_data[i + sequence_length:i + sequence_length + prediction_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 24*30\n",
    "prediction_length = 24*30\n",
    "\n",
    "# Assuming X_data and y_data are already defined and contain the hourly data\n",
    "X, y = create_sequences(X_data, y_data, sequence_length, prediction_length)\n",
    "\n",
    "# Check the shape of the output\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHy4K9fDLrrc",
    "outputId": "8e8f3f3d-4373-48bb-cbea-a30821e3711d"
   },
   "outputs": [],
   "source": [
    "split_index = int(0.9 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "print(\"Training Data Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75bMPp5mMR_v"
   },
   "source": [
    "## Model for MCV correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "6Oa6JD_GMR_w",
    "outputId": "e591b574-e724-4529-f61c-51380ba5d9a5"
   },
   "outputs": [],
   "source": [
    "MCV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aqZJpGsgftS"
   },
   "outputs": [],
   "source": [
    "MCV_df = MCV_df[['MCV Total (MW)','MCV_lag_1','MCV_avg','MCV_7d_avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "85W9O-Ang1fp",
    "outputId": "693e0538-14c6-46c4-a5b3-0b58941f140d"
   },
   "outputs": [],
   "source": [
    "# prompt: graph of correlation matrix of MCV_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming MCV_df is already defined as in your provided code\n",
    "correlation_matrix_mcv = MCV_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_mcv, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of MCV_df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uBh5rpNfzBs"
   },
   "outputs": [],
   "source": [
    "last_month = MCV_df.iloc[-24*31:]\n",
    "MCV_df = MCV_df.iloc[:-24*31]\n",
    "X_df = MCV_df\n",
    "y_df = MCV_df[['MCV Total (MW)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEfWq5X6gBlG"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_df)\n",
    "joblib.dump(scaler_X, f'{today_str}_mcv_month_scaler_X.pkl')\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y_df)\n",
    "joblib.dump(scaler_y, f'{today_str}_mcv_month_scaler_y.pkl')\n",
    "X_data = np.array(X_scaled)\n",
    "y_data = np.array(y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEZ5uunNgGD3",
    "outputId": "29f4ab06-672c-49b6-974a-e1095f22e840"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(X_data, y_data, sequence_length, prediction_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X_data) - sequence_length - prediction_length + 1):\n",
    "        sequences.append(X_data[i:i + sequence_length])\n",
    "        labels.append(y_data[i + sequence_length:i + sequence_length + prediction_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "sequence_length = 24*30\n",
    "prediction_length = 24*30\n",
    "\n",
    "# Assuming X_data and y_data are already defined and contain the hourly data\n",
    "X, y = create_sequences(X_data, y_data, sequence_length, prediction_length)\n",
    "\n",
    "# Check the shape of the output\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUW0bzWpgKDF",
    "outputId": "eaf011ce-534d-4548-ebbc-a546328f55f1"
   },
   "outputs": [],
   "source": [
    "split_index = int(0.9 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "print(\"Training Data Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Mr_uY22MR_w"
   },
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "output_steps = 24*30\n",
    "output_units = output_steps * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMwGeqm-fVTe"
   },
   "source": [
    "##tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "_-koPNf-fTuq",
    "outputId": "bb246508-d11a-4404-90b3-c1942045d41b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(128, return_sequences=True, input_shape=(sequence_length, num_features),\n",
    "                    kernel_regularizer=l2(0.001)))  # L2 regularization\n",
    "model_lstm.add(Dropout(0.2))  # Increased dropout\n",
    "model_lstm.add(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001)))  # Regularization\n",
    "model_lstm.add(Dropout(0.2))  # Increased dropout\n",
    "model_lstm.add(LSTM(32, return_sequences=False, kernel_regularizer=l2(0.001)))  # Regularization\n",
    "model_lstm.add(Dropout(0.2))  # Increased dropout\n",
    "model_lstm.add(Dense(output_units, activation='linear'))\n",
    "\n",
    "# Compile the Model with lower learning rate\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the Model\n",
    "history_lstm = model_lstm.fit(X_train, y_train.reshape(-1, output_units), epochs=50, batch_size=16,\n",
    "                              validation_split=0.3, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Evaluate the Model\n",
    "train_loss_lstm = model_lstm.evaluate(X_train, y_train.reshape(-1, output_units))\n",
    "test_loss_lstm = model_lstm.evaluate(X_test, y_test.reshape(-1, output_units))\n",
    "print(f'Train Loss (LSTM): {train_loss_lstm}')\n",
    "print(f'Test Loss (LSTM): {test_loss_lstm}')\n",
    "\n",
    "# Plot the Training and Validation Loss for LSTM\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss (LSTM)')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss (LSTM)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model_lstm.save('mcV_month_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I37thdHulp58"
   },
   "source": [
    "##HYPERTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jxpd4sjlpfH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import mixed_precision\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QHEn1pPyfjH0",
    "outputId": "51e52c65-5d2f-48db-9aea-c1460ab70301"
   },
   "outputs": [],
   "source": [
    "# Enable mixed precision for better memory usage\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Data generator\n",
    "def data_generator(X, y, batch_size):\n",
    "    \"\"\"Generator to yield batches of data for model training.\"\"\"\n",
    "    num_samples = len(X)\n",
    "    while True:\n",
    "        indices = np.random.permutation(num_samples)  # Shuffle indices\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            yield X_batch, y_batch\n",
    "            del X_batch, y_batch  # Clear batch to free memory\n",
    "\n",
    "# Clear session and run garbage collection after training\n",
    "def clear_session_and_gc():\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# Batch size\n",
    "batch_size = 16  # Reduced batch size for memory efficiency\n",
    "train_gen = data_generator(X_train, y_train.reshape(-1, output_units), batch_size)\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "# Define the LSTM Model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, num_features)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(32, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(16, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(output_units, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Model summary\n",
    "model_lstm.summary()\n",
    "\n",
    "# Train the Model\n",
    "history_lstm = model_lstm.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test.reshape(-1, output_units)),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "train_loss_lstm = model_lstm.evaluate(X_train, y_train.reshape(-1, output_units))\n",
    "test_loss_lstm = model_lstm.evaluate(X_test, y_test.reshape(-1, output_units))\n",
    "\n",
    "print(f'Train Loss (LSTM): {train_loss_lstm}')\n",
    "print(f'Test Loss (LSTM): {test_loss_lstm}')\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss (LSTM)')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss (LSTM)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model_lstm.save('mcv_mcp_lstm_model.h5')\n",
    "\n",
    "# Clear session and garbage collection after training\n",
    "clear_session_and_gc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekPW-S30fTrN",
    "outputId": "c05d1631-ec26-4e02-fd95-12bdcef27420"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values to get the original scale\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))  # Use scaler_y for y_test\n",
    "y_pred_lstm_original = scaler_y.inverse_transform(y_pred_lstm.reshape(-1, 1))  # Use scaler_y for predictions\n",
    "\n",
    "# Calculate the R-squared score\n",
    "r2_lstm = r2_score(y_test_original, y_pred_lstm_original)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_lstm = mean_squared_error(y_test_original, y_pred_lstm_original)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"LSTM - R-squared: {r2_lstm}, MSE: {mse_lstm}, RMSE: {rmse_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "LB7_A4ZKfTon",
    "outputId": "5165fb27-7f86-4b88-e4b9-5524334c6a9e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test_original and y_pred_lstm_original are already defined from your previous code\n",
    "\n",
    "# Select the last 672 entries (representing the last week)\n",
    "last_week_actual = y_test_original[-24*30:]\n",
    "last_week_predicted = y_pred_lstm_original[-24*30:]\n",
    "\n",
    "# Plot the graph for the last week\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(last_week_actual, label='Actual Values')\n",
    "plt.plot(last_week_predicted, label='Predicted Values')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Scaled Values')\n",
    "plt.title('Actual vs. Predicted Values for (X_test)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UTgyOSF87gw"
   },
   "source": [
    "##LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPz9bWBnaXMK",
    "outputId": "22f42c6f-c02e-414a-f49d-cb1b1b98b67e"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "model_lstm = Sequential()\n",
    "#model_lstm.add(LSTM(256, return_sequences=True, input_shape=(sequence_length, num_features)))\n",
    "#model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(LSTM(128, return_sequences=True, input_shape=(sequence_length, num_features)))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(LSTM(64, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(LSTM(32, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(Dense(output_units, activation='linear'))  # MCP and MCV combined output\n",
    "\n",
    "# Compile the Model\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Model Summary\n",
    "#model_lstm.summary()\n",
    "\n",
    "# Train the Model\n",
    "history_lstm = model_lstm.fit(X_train, y_train.reshape(-1, output_units), epochs=50, batch_size=16,\n",
    "                              validation_split=0.3, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the Model\n",
    "train_loss_lstm = model_lstm.evaluate(X_train, y_train.reshape(-1, output_units))\n",
    "test_loss_lstm = model_lstm.evaluate(X_test, y_test.reshape(-1, output_units))\n",
    "print(f'Train Loss (LSTM): {train_loss_lstm}')\n",
    "print(f'Test Loss (LSTM): {test_loss_lstm}')\n",
    "\n",
    "# Plot the Training and Validation Loss for LSTM\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss (LSTM)')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss (LSTM)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model_lstm.save('mcv_mcp_lstm_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "U9QTDDOS8yAY",
    "MFMK5LCiaZV2",
    "gfF0Hm5LLmRj"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
